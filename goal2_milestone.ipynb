{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac996b0b-2627-416f-bed5-c5548aa746c0",
   "metadata": {},
   "source": [
    "# 324321214214122141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945c2260-1b88-4a68-af1f-0286d57ac8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths (Match your notebook)\n",
    "FOLDER_PATH = \"/home/juan/Work/Midterm project/splited/\"\n",
    "X_PATH = os.path.join(FOLDER_PATH, \"X_data.npz\")\n",
    "Y_PATH = os.path.join(FOLDER_PATH, \"Y_data.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf46dff9-3494-4107-bb3e-acc4929d0ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Features: (661271, 5286)\n",
      "Targets:  (661271, 10488)\n",
      "Data successfully split.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Data...\")\n",
    "X = sparse.load_npz(X_PATH)\n",
    "Y = sparse.load_npz(Y_PATH)\n",
    "\n",
    "print(f\"Features: {X.shape}\")\n",
    "print(f\"Targets:  {Y.shape}\")  # Should be (661271, 10488)\n",
    "\n",
    "# Split keeping sparse matrix to save RAM\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print(\"Data successfully split.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "387901c2-4357-4dc3-b654-7ef639d6dd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal Loss initialized.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # BCE with logits\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        \n",
    "        # pt is the probability of being right\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        \n",
    "        # Focal term: (1-pt)^gamma. \n",
    "        # If pt is high (easy example), this term approaches 0.\n",
    "        # If pt is low (hard example), this term stays high.\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean': return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum': return torch.sum(F_loss)\n",
    "        else: return F_loss\n",
    "\n",
    "print(\"Focal Loss initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103c72ba-2c61-45ae-84b5-6b0aa431edb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepDrugResNet Architecture defined.\n"
     ]
    }
   ],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The \"Residual\" connection: x + block(x)\n",
    "        return x + self.block(x)\n",
    "\n",
    "class DeepDrugResNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DeepDrugResNet, self).__init__()\n",
    "        \n",
    "        # 1. Project Input up to Hidden Dimension\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # 2. Residual Blocks (Deep Learning Magic)\n",
    "        self.res1 = ResidualBlock(2048)\n",
    "        self.res2 = ResidualBlock(2048)\n",
    "        \n",
    "        # 3. Bottleneck to Output\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(1024, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.bottleneck(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "print(\"DeepDrugResNet Architecture defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53941631-b4e5-4e76-a2b0-657fc521f97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders ready.\n"
     ]
    }
   ],
   "source": [
    "class DrugMultilabelDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert only one row to dense at a time\n",
    "        x_row = torch.tensor(self.X[idx].toarray(), dtype=torch.float32).squeeze()\n",
    "        y_row = torch.tensor(self.Y[idx].toarray(), dtype=torch.float32).squeeze()\n",
    "        return x_row, y_row\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 128  # Increase to 256 or 512 if your GPU has >8GB VRAM\n",
    "train_ds = DrugMultilabelDataset(X_train, Y_train)\n",
    "test_ds = DrugMultilabelDataset(X_test, Y_test)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "print(\"DataLoaders ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0742c68-fedc-4819-8af4-52c153409f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Deep ResNet on cuda...\n",
      "Epoch 1/10 | Loss: 0.00040 | LR: 0.001000\n",
      "Epoch 2/10 | Loss: 0.00030 | LR: 0.001000\n",
      "Epoch 3/10 | Loss: 0.00029 | LR: 0.001000\n",
      "Epoch 4/10 | Loss: 0.00028 | LR: 0.001000\n",
      "Epoch 5/10 | Loss: 0.00027 | LR: 0.001000\n",
      "Epoch 6/10 | Loss: 0.00027 | LR: 0.001000\n",
      "Epoch 7/10 | Loss: 0.00027 | LR: 0.001000\n",
      "Epoch 8/10 | Loss: 0.00026 | LR: 0.001000\n",
      "Epoch 9/10 | Loss: 0.00026 | LR: 0.001000\n",
      "Epoch 10/10 | Loss: 0.00025 | LR: 0.001000\n",
      "Training Complete.\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup Model\n",
    "input_size = X.shape[1]\n",
    "output_size = Y.shape[1]\n",
    "model = DeepDrugResNet(input_size, output_size).to(device)\n",
    "\n",
    "# 2. Setup Smart Loss\n",
    "# We use Focal Loss which handles imbalance automatically\n",
    "criterion = FocalLoss(alpha=0.75, gamma=2) \n",
    "\n",
    "# 3. Optimizer with Weight Decay (Regularization)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# 4. Scheduler (FIXED: Removed 'verbose=True')\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.1, \n",
    "    patience=1\n",
    ")\n",
    "\n",
    "# 5. Training Loop\n",
    "epochs = 10 \n",
    "print(f\"Training Deep ResNet on {device}...\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (xb, yb) in enumerate(train_dl):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    \n",
    "    # Get current Learning Rate manually\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.5f} | LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Update learning rate based on loss\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a437fc9e-93de-40ee-8678-54197ffc441d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming evaluation (Threshold=0.5)...\n",
      "\n",
      "--- Test Set Results (Memory Efficient) ---\n",
      "Micro Precision: 0.5264\n",
      "Micro Recall:    0.0240\n",
      "Micro F1 Score:  0.0459\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_and_log_memory_efficient(model, dataloader, threshold=0.2):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Initialize Counters (on GPU to be fast)\n",
    "    # These take almost 0 RAM compared to storing the whole dataset\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "    \n",
    "    # Store counts for plotting later (Size: 10488, very small)\n",
    "    pred_counts = torch.zeros(output_size).to(device)\n",
    "    true_counts = torch.zeros(output_size).to(device)\n",
    "    \n",
    "    print(f\"Streaming evaluation (Threshold={threshold})...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device) # Move targets to GPU for fast math\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(xb)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > threshold).float()\n",
    "            \n",
    "            # --- RAM SAVING MAGIC STARTS HERE ---\n",
    "            # Instead of saving 'preds', we calculate stats immediately & discard data\n",
    "            \n",
    "            # 1. Update Metrics Stats (Micro-Average logic)\n",
    "            total_tp += (preds * yb).sum().item()       # True Positives\n",
    "            total_fp += (preds * (1 - yb)).sum().item() # False Positives\n",
    "            total_fn += ((1 - preds) * yb).sum().item() # False Negatives\n",
    "            \n",
    "            # 2. Update Plotting Stats (Sum of events)\n",
    "            pred_counts += preds.sum(dim=0)\n",
    "            true_counts += yb.sum(dim=0)\n",
    "            \n",
    "            # Python automatically frees 'xb', 'preds', etc. here\n",
    "            # --- END OF BATCH ---\n",
    "\n",
    "    # Calculate Final Metrics\n",
    "    epsilon = 1e-7\n",
    "    precision = total_tp / (total_tp + total_fp + epsilon)\n",
    "    recall = total_tp / (total_tp + total_fn + epsilon)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "    \n",
    "    print(\"\\n--- Test Set Results (Memory Efficient) ---\")\n",
    "    print(f\"Micro Precision: {precision:.4f}\")\n",
    "    print(f\"Micro Recall:    {recall:.4f}\")\n",
    "    print(f\"Micro F1 Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Move counts to CPU for plotting\n",
    "    return true_counts.cpu().numpy(), pred_counts.cpu().numpy()\n",
    "\n",
    "# Execute\n",
    "# Note: We do NOT get 'y_pred' back because it's too big for RAM. \n",
    "# We get 'counts' instead which allows us to make the plots.\n",
    "true_counts, pred_counts = evaluate_and_log_memory_efficient(model, test_dl, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daa895e2-a076-4fff-9aaf-b97352e16c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Precision@100: 1.92%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_precision_at_k(model, dataloader, k=10):\n",
    "    model.eval()\n",
    "    total_precision = 0.0\n",
    "    num_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            \n",
    "            # Get raw logits (ranking scores)\n",
    "            logits = model(xb)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            \n",
    "            # Get the top K highest probability indices\n",
    "            # We don't care if prob is 0.01 or 0.99, just the order\n",
    "            _, top_indices = torch.topk(probs, k=k, dim=1)\n",
    "            \n",
    "            # Check if actual truth (1) exists at those indices\n",
    "            # gather() pulls the values from yb at the positions in top_indices\n",
    "            relevant = torch.gather(yb, 1, top_indices)\n",
    "            \n",
    "            # Average correct in top K\n",
    "            batch_p_at_k = relevant.sum().item() / (k * xb.size(0))\n",
    "            \n",
    "            # Accumulate (weighted by batch size)\n",
    "            total_precision += batch_p_at_k * xb.size(0)\n",
    "            num_samples += xb.size(0)\n",
    "\n",
    "    score = total_precision / num_samples\n",
    "    print(f\"\\n Precision@{k}: {score:.2%}\")\n",
    "\n",
    "# Run it\n",
    "evaluate_precision_at_k(model, test_dl, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcea3e3-026b-4163-893a-933048f6ade3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
