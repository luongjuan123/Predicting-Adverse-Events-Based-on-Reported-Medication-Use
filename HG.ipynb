{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b322f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dotie\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- B∆∞·ªõc 1: Qu√©t t√™n thu·ªëc v√† ph·∫£n ·ª©ng ƒë·ªÉ t·∫°o ID ---\n",
      "S·ªë l∆∞·ª£ng thu·ªëc: 5259\n",
      "S·ªë l∆∞·ª£ng ph·∫£n ·ª©ng: 10488\n",
      "--- B∆∞·ªõc 2: ƒê·ªçc d·ªØ li·ªáu v√† t·∫°o danh s√°ch c·∫°nh ---\n",
      "Processing: processed_final_1.csv\n",
      "Processing: processed_final_10.csv\n",
      "Processing: processed_final_100.csv\n",
      "Processing: processed_final_101.csv\n",
      "Processing: processed_final_102.csv\n",
      "Processing: processed_final_103.csv\n",
      "Processing: processed_final_104.csv\n",
      "Processing: processed_final_105.csv\n",
      "Processing: processed_final_106.csv\n",
      "Processing: processed_final_107.csv\n",
      "Processing: processed_final_108.csv\n",
      "Processing: processed_final_109.csv\n",
      "Processing: processed_final_11.csv\n",
      "Processing: processed_final_110.csv\n",
      "Processing: processed_final_111.csv\n",
      "Processing: processed_final_112.csv\n",
      "Processing: processed_final_113.csv\n",
      "Processing: processed_final_114.csv\n",
      "Processing: processed_final_115.csv\n",
      "Processing: processed_final_116.csv\n",
      "Processing: processed_final_117.csv\n",
      "Processing: processed_final_118.csv\n",
      "Processing: processed_final_119.csv\n",
      "Processing: processed_final_12.csv\n",
      "Processing: processed_final_120.csv\n",
      "Processing: processed_final_121.csv\n",
      "Processing: processed_final_122.csv\n",
      "Processing: processed_final_123.csv\n",
      "Processing: processed_final_124.csv\n",
      "Processing: processed_final_125.csv\n",
      "Processing: processed_final_126.csv\n",
      "Processing: processed_final_127.csv\n",
      "Processing: processed_final_128.csv\n",
      "Processing: processed_final_129.csv\n",
      "Processing: processed_final_13.csv\n",
      "Processing: processed_final_130.csv\n",
      "Processing: processed_final_131.csv\n",
      "Processing: processed_final_132.csv\n",
      "Processing: processed_final_133.csv\n",
      "Processing: processed_final_14.csv\n",
      "Processing: processed_final_15.csv\n",
      "Processing: processed_final_16.csv\n",
      "Processing: processed_final_17.csv\n",
      "Processing: processed_final_18.csv\n",
      "Processing: processed_final_19.csv\n",
      "Processing: processed_final_2.csv\n",
      "Processing: processed_final_20.csv\n",
      "Processing: processed_final_21.csv\n",
      "Processing: processed_final_22.csv\n",
      "Processing: processed_final_23.csv\n",
      "Processing: processed_final_24.csv\n",
      "Processing: processed_final_25.csv\n",
      "Processing: processed_final_26.csv\n",
      "Processing: processed_final_27.csv\n",
      "Processing: processed_final_28.csv\n",
      "Processing: processed_final_29.csv\n",
      "Processing: processed_final_3.csv\n",
      "Processing: processed_final_30.csv\n",
      "Processing: processed_final_31.csv\n",
      "Processing: processed_final_32.csv\n",
      "Processing: processed_final_33.csv\n",
      "Processing: processed_final_34.csv\n",
      "Processing: processed_final_35.csv\n",
      "Processing: processed_final_36.csv\n",
      "Processing: processed_final_37.csv\n",
      "Processing: processed_final_38.csv\n",
      "Processing: processed_final_39.csv\n",
      "Processing: processed_final_4.csv\n",
      "Processing: processed_final_40.csv\n",
      "Processing: processed_final_41.csv\n",
      "Processing: processed_final_42.csv\n",
      "Processing: processed_final_43.csv\n",
      "Processing: processed_final_44.csv\n",
      "Processing: processed_final_45.csv\n",
      "Processing: processed_final_46.csv\n",
      "Processing: processed_final_47.csv\n",
      "Processing: processed_final_48.csv\n",
      "Processing: processed_final_49.csv\n",
      "Processing: processed_final_5.csv\n",
      "Processing: processed_final_50.csv\n",
      "Processing: processed_final_51.csv\n",
      "Processing: processed_final_52.csv\n",
      "Processing: processed_final_53.csv\n",
      "Processing: processed_final_54.csv\n",
      "Processing: processed_final_55.csv\n",
      "Processing: processed_final_56.csv\n",
      "Processing: processed_final_57.csv\n",
      "Processing: processed_final_58.csv\n",
      "Processing: processed_final_59.csv\n",
      "Processing: processed_final_6.csv\n",
      "Processing: processed_final_60.csv\n",
      "Processing: processed_final_61.csv\n",
      "Processing: processed_final_62.csv\n",
      "Processing: processed_final_63.csv\n",
      "Processing: processed_final_64.csv\n",
      "Processing: processed_final_65.csv\n",
      "Processing: processed_final_66.csv\n",
      "Processing: processed_final_67.csv\n",
      "Processing: processed_final_68.csv\n",
      "Processing: processed_final_69.csv\n",
      "Processing: processed_final_7.csv\n",
      "Processing: processed_final_70.csv\n",
      "Processing: processed_final_71.csv\n",
      "Processing: processed_final_72.csv\n",
      "Processing: processed_final_73.csv\n",
      "Processing: processed_final_74.csv\n",
      "Processing: processed_final_75.csv\n",
      "Processing: processed_final_76.csv\n",
      "Processing: processed_final_77.csv\n",
      "Processing: processed_final_78.csv\n",
      "Processing: processed_final_79.csv\n",
      "Processing: processed_final_8.csv\n",
      "Processing: processed_final_80.csv\n",
      "Processing: processed_final_81.csv\n",
      "Processing: processed_final_82.csv\n",
      "Processing: processed_final_83.csv\n",
      "Processing: processed_final_84.csv\n",
      "Processing: processed_final_85.csv\n",
      "Processing: processed_final_86.csv\n",
      "Processing: processed_final_87.csv\n",
      "Processing: processed_final_88.csv\n",
      "Processing: processed_final_89.csv\n",
      "Processing: processed_final_9.csv\n",
      "Processing: processed_final_90.csv\n",
      "Processing: processed_final_91.csv\n",
      "Processing: processed_final_92.csv\n",
      "Processing: processed_final_93.csv\n",
      "Processing: processed_final_94.csv\n",
      "Processing: processed_final_95.csv\n",
      "Processing: processed_final_96.csv\n",
      "Processing: processed_final_97.csv\n",
      "Processing: processed_final_98.csv\n",
      "Processing: processed_final_99.csv\n",
      "ƒêang gh√©p n·ªëi d·ªØ li·ªáu...\n",
      "--- B∆∞·ªõc 3: T·∫°o Graph Object ---\n",
      "Th√¥ng tin ƒë·ªì th·ªã: HeteroData(\n",
      "  patient={\n",
      "    x=[661284, 2],\n",
      "    num_nodes=661284,\n",
      "  },\n",
      "  drug={ num_nodes=5259 },\n",
      "  reaction={ num_nodes=10488 },\n",
      "  (patient, takes, drug)={ edge_index=[2, 1646336] },\n",
      "  (patient, has_reaction, reaction)={ edge_index=[2, 1652565] },\n",
      "  (drug, taken_by, patient)={ edge_index=[2, 1646336] },\n",
      "  (reaction, reaction_in, patient)={ edge_index=[2, 1652565] }\n",
      ")\n",
      "‚úÖ ƒê√£ l∆∞u file 'hetero_graph_data.pt'. Xong ph·∫ßn chu·∫©n b·ªã!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "input_dir = r\"C:\\Users\\dotie\\OneDrive\\Documents\\FAERS\\Processed_Chunks\"\n",
    "all_files = glob.glob(os.path.join(input_dir, \"processed_final_*.csv\"))\n",
    "\n",
    "# --- 1. X√ÇY D·ª∞NG T·ª™ ƒêI·ªÇN ID (MAPPING) ---\n",
    "# ƒê·ªÉ ti·∫øt ki·ªám RAM, ta ch·ªâ qu√©t t√™n c·ªôt (v·ªën ch·ª©a t√™n thu·ªëc/b·ªánh) ch·ª© kh√¥ng ƒë·ªçc to√†n b·ªô d·ªØ li·ªáu\n",
    "print(\"--- B∆∞·ªõc 1: Qu√©t t√™n thu·ªëc v√† ph·∫£n ·ª©ng ƒë·ªÉ t·∫°o ID ---\")\n",
    "\n",
    "# ƒê·ªçc file ƒë·∫ßu ti√™n ƒë·ªÉ l·∫•y danh s√°ch c·ªôt (V√¨ c√°c file ƒë√£ ƒë·ªìng b·ªô c·ªôt)\n",
    "df_cols = pd.read_csv(all_files[0], nrows=0) \n",
    "drug_names = [c for c in df_cols.columns if c.startswith('x ') and c not in ['x Age', 'x Gender', 'x AgeGroup']]\n",
    "reaction_names = [c for c in df_cols.columns if c.startswith('y ')]\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng thu·ªëc: {len(drug_names)}\")\n",
    "print(f\"S·ªë l∆∞·ª£ng ph·∫£n ·ª©ng: {len(reaction_names)}\")\n",
    "\n",
    "# --- 2. X√ÇY D·ª∞NG DANH S√ÅCH C·∫†NH (EDGE LIST) ---\n",
    "print(\"--- B∆∞·ªõc 2: ƒê·ªçc d·ªØ li·ªáu v√† t·∫°o danh s√°ch c·∫°nh ---\")\n",
    "\n",
    "# Kh·ªüi t·∫°o list ch·ª©a c·∫°nh\n",
    "src_patient_drug = []\n",
    "dst_patient_drug = []\n",
    "src_patient_react = []\n",
    "dst_patient_react = []\n",
    "\n",
    "patient_features_list = []\n",
    "global_patient_id = 0 # ID b·ªánh nh√¢n tƒÉng d·∫ßn\n",
    "\n",
    "for f in all_files:\n",
    "    # ƒê·ªçc t·ª´ng chunk\n",
    "    print(f\"Processing: {os.path.basename(f)}\")\n",
    "    df = pd.read_csv(f)\n",
    "    \n",
    "    num_rows = len(df)\n",
    "    local_patient_ids = np.arange(num_rows) + global_patient_id\n",
    "    \n",
    "    # 1. L·∫•y ƒë·∫∑c tr∆∞ng b·ªánh nh√¢n (Age, Gender)\n",
    "    # Normalize Age: chia 100\n",
    "    p_feats = df[['x Age', 'x Gender']].values\n",
    "    p_feats[:, 0] = p_feats[:, 0] / 100.0 \n",
    "    patient_features_list.append(p_feats)\n",
    "    \n",
    "    # 2. T·∫°o c·∫°nh Patient -> Drug\n",
    "    # L·∫•y ma tr·∫≠n con c·ªßa thu·ªëc (d·∫°ng dense)\n",
    "    # L∆∞u √Ω: N·∫øu m√°y y·∫øu, ƒëo·∫°n n√†y c√≥ th·ªÉ l√†m t·ª´ng d√≤ng, nh∆∞ng numpy vectorization nhanh h∆°n\n",
    "    drug_matrix = df[drug_names].values\n",
    "    # T√¨m c√°c v·ªã tr√≠ thu·ªëc > 0\n",
    "    row_indices, col_indices = np.where(drug_matrix > 0)\n",
    "    \n",
    "    # Map row index local -> global patient ID\n",
    "    src_patient_drug.append(local_patient_ids[row_indices])\n",
    "    dst_patient_drug.append(col_indices) # col_indices ch√≠nh l√† ID c·ªßa thu·ªëc (do th·ª© t·ª± c·ªôt c·ªë ƒë·ªãnh)\n",
    "    \n",
    "    # 3. T·∫°o c·∫°nh Patient -> Reaction\n",
    "    react_matrix = df[reaction_names].values\n",
    "    row_indices_r, col_indices_r = np.where(react_matrix > 0)\n",
    "    \n",
    "    src_patient_react.append(local_patient_ids[row_indices_r])\n",
    "    dst_patient_react.append(col_indices_r)\n",
    "    \n",
    "    global_patient_id += num_rows\n",
    "    \n",
    "    # D·ªçn RAM\n",
    "    del df, drug_matrix, react_matrix\n",
    "    gc.collect()\n",
    "\n",
    "# G·ªôp c√°c list l·∫°i th√†nh numpy array l·ªõn\n",
    "print(\"ƒêang gh√©p n·ªëi d·ªØ li·ªáu...\")\n",
    "patient_features = np.vstack(patient_features_list)\n",
    "edge_index_drug = np.vstack([np.concatenate(src_patient_drug), np.concatenate(dst_patient_drug)])\n",
    "edge_index_react = np.vstack([np.concatenate(src_patient_react), np.concatenate(dst_patient_react)])\n",
    "\n",
    "# --- 3. T·∫†O HETERODATA ---\n",
    "print(\"--- B∆∞·ªõc 3: T·∫°o Graph Object ---\")\n",
    "data = HeteroData()\n",
    "\n",
    "# Nodes\n",
    "data['patient'].x = torch.from_numpy(patient_features).float()\n",
    "data['patient'].num_nodes = global_patient_id\n",
    "# Drug & Reaction kh√¥ng c·∫ßn features input, ta s·∫Ω d√πng Embedding Layer trong model\n",
    "data['drug'].num_nodes = len(drug_names)\n",
    "data['reaction'].num_nodes = len(reaction_names)\n",
    "\n",
    "# Edges (Chuy·ªÉn sang LongTensor)\n",
    "data['patient', 'takes', 'drug'].edge_index = torch.from_numpy(edge_index_drug).long()\n",
    "data['patient', 'has_reaction', 'reaction'].edge_index = torch.from_numpy(edge_index_react).long()\n",
    "\n",
    "# T·∫°o c·∫°nh ng∆∞·ª£c (Reverse edges) cho GNN\n",
    "data['drug', 'taken_by', 'patient'].edge_index = torch.flip(data['patient', 'takes', 'drug'].edge_index, [0])\n",
    "data['reaction', 'reaction_in', 'patient'].edge_index = torch.flip(data['patient', 'has_reaction', 'reaction'].edge_index, [0])\n",
    "\n",
    "print(\"Th√¥ng tin ƒë·ªì th·ªã:\", data)\n",
    "\n",
    "# L∆∞u xu·ªëng ·ªï c·ª©ng ƒë·ªÉ d√πng cho file train\n",
    "torch.save(data, \"hetero_graph_data.pt\")\n",
    "print(\"‚úÖ ƒê√£ l∆∞u file 'hetero_graph_data.pt'. Xong ph·∫ßn chu·∫©n b·ªã!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e48b42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dotie\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T√¨m th·∫•y 133 file d·ªØ li·ªáu.\n",
      "S·ªë l∆∞·ª£ng thu·ªëc (Input Nodes): 5259\n",
      "S·ªë l∆∞·ª£ng ph·∫£n ·ª©ng (Output Nodes): 10488\n",
      "B·∫Øt ƒë·∫ßu training (File-by-File)...\n",
      "\n",
      "================ EPOCH 1 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dotie\\AppData\\Local\\Temp\\ipykernel_8496\\1847126495.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  edge_index_drug = torch.tensor([rows, cols], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 0/133 files. Loss: 0.8604\n",
      "  Processed 10/133 files. Loss: 0.4947\n",
      "  Processed 20/133 files. Loss: 0.4974\n",
      "  Processed 30/133 files. Loss: 0.5369\n",
      "  Processed 40/133 files. Loss: 0.4246\n",
      "  Processed 50/133 files. Loss: 0.4147\n",
      "  Processed 60/133 files. Loss: 0.3669\n",
      "  Processed 70/133 files. Loss: 0.4295\n",
      "  Processed 80/133 files. Loss: 0.4584\n",
      "  Processed 90/133 files. Loss: 0.4431\n",
      "  Processed 100/133 files. Loss: 0.3799\n",
      "  Processed 110/133 files. Loss: 0.4427\n",
      "  Processed 120/133 files. Loss: 0.4152\n",
      "  Processed 130/133 files. Loss: 0.4122\n",
      "‚úÖ K·∫æT TH√öC EPOCH 1\n",
      "   - Average Loss: 0.4446\n",
      "   - Precision:    0.9187\n",
      "   - Recall:       0.9732\n",
      "   - F1-Score:     0.9451\n",
      "\n",
      "üéâ ƒê√£ train xong! (Model saved)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Reaction_Names.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import SAGEConv, to_hetero, Linear\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. C·∫§U H√åNH & CHU·∫®N B·ªä\n",
    "# ---------------------------------------------------------\n",
    "input_dir = r\"C:\\Users\\dotie\\OneDrive\\Documents\\FAERS\\Processed_Chunks\"\n",
    "all_files = glob.glob(os.path.join(input_dir, \"processed_final_*.csv\"))\n",
    "\n",
    "print(f\"T√¨m th·∫•y {len(all_files)} file d·ªØ li·ªáu.\")\n",
    "\n",
    "# ƒê·ªçc file ƒë·∫ßu ti√™n ƒë·ªÉ l·∫•y th√¥ng tin k√≠ch th∆∞·ªõc (S·ªë l∆∞·ª£ng thu·ªëc/ph·∫£n ·ª©ng to√†n c·ª•c)\n",
    "df_temp = pd.read_csv(all_files[0], nrows=1)\n",
    "drug_cols = [c for c in df_temp.columns if c.startswith('x ') and c not in ['x Age', 'x Gender', 'x AgeGroup']]\n",
    "reaction_cols = [c for c in df_temp.columns if c.startswith('y ')]\n",
    "num_drugs = len(drug_cols)\n",
    "num_reactions = len(reaction_cols)\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng thu·ªëc (Input Nodes): {num_drugs}\")\n",
    "print(f\"S·ªë l∆∞·ª£ng ph·∫£n ·ª©ng (Output Nodes): {num_reactions}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. ƒê·ªäNH NGHƒ®A MODEL\n",
    "# ---------------------------------------------------------\n",
    "class HeteroSage(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = SAGEConv((-1, -1), hidden_channels)\n",
    "            self.convs.append(conv)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index).relu()\n",
    "        return x\n",
    "\n",
    "# Metadata gi·∫£ l·∫≠p ƒë·ªÉ kh·ªüi t·∫°o model (c·∫•u tr√∫c ƒë·ªì th·ªã)\n",
    "# Ta c·∫ßn khai b√°o c√°c lo·∫°i node v√† edge s·∫Ω xu·∫•t hi·ªán\n",
    "metadata = (\n",
    "    ['patient', 'drug', 'reaction'],\n",
    "    [('patient', 'takes', 'drug'), \n",
    "     ('drug', 'taken_by', 'patient'),\n",
    "     ('patient', 'has_reaction', 'reaction'),\n",
    "     ('reaction', 'reaction_in', 'patient')]\n",
    ")\n",
    "\n",
    "model = HeteroSage(hidden_channels=64, out_channels=64, num_layers=2)\n",
    "model = to_hetero(model, metadata, aggr='sum')\n",
    "\n",
    "# Embedding layers cho Drug v√† Reaction\n",
    "# +1 ƒë·ªÉ d·ª± ph√≤ng index\n",
    "drug_emb = torch.nn.Embedding(num_drugs + 1, 64)\n",
    "react_emb = torch.nn.Embedding(num_reactions + 1, 64)\n",
    "# Linear layer cho Patient (Age, Gender)\n",
    "patient_lin = Linear(2, 64)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model.parameters()) + list(drug_emb.parameters()) + list(react_emb.parameters()) + list(patient_lin.parameters()), \n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. H√ÄM CHUY·ªÇN ƒê·ªîI DATAFRAME -> GRAPH (Mini-Graph)\n",
    "# ---------------------------------------------------------\n",
    "def create_mini_graph(df):\n",
    "    # 1. Patient Features\n",
    "    p_feats = df[['x Age', 'x Gender']].values\n",
    "    p_feats[:, 0] = p_feats[:, 0] / 100.0 \n",
    "    x_patient = torch.tensor(p_feats, dtype=torch.float)\n",
    "    \n",
    "    # 2. Edges: Patient -> Drug\n",
    "    # D√πng numpy where ƒë·ªÉ t√¨m c√°c √¥ c√≥ gi√° tr·ªã > 0 (ng∆∞·ªùi d√πng thu·ªëc)\n",
    "    # drug_cols ƒë√£ ƒë∆∞·ª£c sort v√† c·ªë ƒë·ªãnh v·ªã tr√≠ ·ªü b∆∞·ªõc chu·∫©n b·ªã\n",
    "    drug_matrix = df[drug_cols].values\n",
    "    rows, cols = np.where(drug_matrix > 0)\n",
    "    edge_index_drug = torch.tensor([rows, cols], dtype=torch.long)\n",
    "    \n",
    "    # 3. Edges: Patient -> Reaction\n",
    "    react_matrix = df[reaction_cols].values\n",
    "    rows_r, cols_r = np.where(react_matrix > 0)\n",
    "    edge_index_react = torch.tensor([rows_r, cols_r], dtype=torch.long)\n",
    "    \n",
    "    # T·∫°o HeteroData\n",
    "    data = HeteroData()\n",
    "    data['patient'].x = x_patient\n",
    "    data['patient'].num_nodes = len(df)\n",
    "    \n",
    "    # G√°n Embedding (ID) cho thu·ªëc/ph·∫£n ·ª©ng\n",
    "    # ·ªû ƒë√¢y ta truy·ªÅn v√†o indices t·ª´ 0 -> num_drugs ƒë·ªÉ Embedding layer tra c·ª©u\n",
    "    data['drug'].x = torch.arange(num_drugs)\n",
    "    data['reaction'].x = torch.arange(num_reactions)\n",
    "    \n",
    "    data['patient', 'takes', 'drug'].edge_index = edge_index_drug\n",
    "    data['patient', 'has_reaction', 'reaction'].edge_index = edge_index_react\n",
    "    \n",
    "    # C·∫°nh ng∆∞·ª£c\n",
    "    data['drug', 'taken_by', 'patient'].edge_index = torch.flip(edge_index_drug, [0])\n",
    "    data['reaction', 'reaction_in', 'patient'].edge_index = torch.flip(edge_index_react, [0])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. TRAINING LOOP (Chunk-based)\n",
    "# ---------------------------------------------------------\n",
    "print(\"B·∫Øt ƒë·∫ßu training (File-by-File)...\")\n",
    "\n",
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    # Bi·∫øn ƒë·ªÉ t√≠nh metrics to√†n c·ª•c cho Epoch\n",
    "    total_tp = 0 # True Positives\n",
    "    total_fp = 0 # False Positives\n",
    "    total_fn = 0 # False Negatives\n",
    "    \n",
    "    # Duy·ªát qua t·ª´ng file CSV\n",
    "    for i, f_path in enumerate(all_files):\n",
    "        try:\n",
    "            df_chunk = pd.read_csv(f_path)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        batch = create_mini_graph(df_chunk)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # --- Forward Pass ---\n",
    "        x_dict = {\n",
    "            'patient': patient_lin(batch['patient'].x),\n",
    "            'drug': drug_emb(batch['drug'].x),\n",
    "            'reaction': react_emb(batch['reaction'].x)\n",
    "        }\n",
    "        \n",
    "        out = model(x_dict, batch.edge_index_dict)\n",
    "        \n",
    "        edge_index = batch['patient', 'has_reaction', 'reaction'].edge_index\n",
    "        if edge_index.numel() == 0: continue\n",
    "\n",
    "        src_emb = out['patient'][edge_index[0]]\n",
    "        dst_emb = out['reaction'][edge_index[1]]\n",
    "        \n",
    "        # --- T√≠nh ƒëi·ªÉm (Logits) ---\n",
    "        pos_score = (src_emb * dst_emb).sum(dim=-1) # Logits cho m·∫´u D∆∞∆°ng (C√≥ th·∫≠t)\n",
    "        \n",
    "        neg_dst_idx = torch.randint(0, num_reactions, (len(pos_score),))\n",
    "        neg_dst_emb = out['reaction'][neg_dst_idx]\n",
    "        neg_score = (src_emb * neg_dst_emb).sum(dim=-1) # Logits cho m·∫´u √Çm (Gi·∫£)\n",
    "        \n",
    "        # --- T√≠nh Loss ---\n",
    "        scores = torch.cat([pos_score, neg_score])\n",
    "        labels = torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # --- T√çNH TO√ÅN METRICS CHO BATCH N√ÄY ---\n",
    "        # Ng∆∞·ª°ng ph√¢n lo·∫°i l√† 0 (v√¨ Sigmoid(0) = 0.5)\n",
    "        # S·ª≠ d·ª•ng torch.no_grad() ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn gradient\n",
    "        with torch.no_grad():\n",
    "            # TP: M·∫´u d∆∞∆°ng c√≥ ƒëi·ªÉm d·ª± ƒëo√°n > 0\n",
    "            tp = (pos_score > 0).sum().item()\n",
    "            # FN: M·∫´u d∆∞∆°ng c√≥ ƒëi·ªÉm d·ª± ƒëo√°n <= 0\n",
    "            fn = (pos_score <= 0).sum().item()\n",
    "            # FP: M·∫´u √¢m c√≥ ƒëi·ªÉm d·ª± ƒëo√°n > 0\n",
    "            fp = (neg_score > 0).sum().item()\n",
    "            \n",
    "            total_tp += tp\n",
    "            total_fp += fp\n",
    "            total_fn += fn\n",
    "        \n",
    "        batch_size = len(df_chunk)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_examples += batch_size\n",
    "        \n",
    "        del df_chunk, batch\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"  Processed {i}/{len(all_files)} files. Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # --- T√çNH METRICS T·ªîNG H·ª¢P CU·ªêI EPOCH ---\n",
    "    epsilon = 1e-9 # S·ªë nh·ªè ƒë·ªÉ tr√°nh l·ªói chia cho 0\n",
    "    \n",
    "    # 1. Precision = TP / (TP + FP)\n",
    "    # (Trong s·ªë nh·ªØng c√°i m√°y ƒëo√°n l√† c√≥ ph·∫£n ·ª©ng, bao nhi√™u % l√† ƒë√∫ng?)\n",
    "    precision = total_tp / (total_tp + total_fp + epsilon)\n",
    "    \n",
    "    # 2. Recall = TP / (TP + FN)\n",
    "    # (Trong s·ªë c√°c ph·∫£n ·ª©ng th·ª±c t·∫ø, m√°y b·∫Øt ƒë∆∞·ª£c bao nhi√™u %?)\n",
    "    recall = total_tp / (total_tp + total_fn + epsilon)\n",
    "    \n",
    "    # 3. F1-Score = 2 * (P * R) / (P + R)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "    \n",
    "    avg_loss = total_loss / (total_examples + epsilon)\n",
    "    \n",
    "    return avg_loss, precision, recall, f1\n",
    "\n",
    "    # Ch·∫°y Training\n",
    "for epoch in range(1, 2): # V√≠ d·ª• ch·∫°y 10 Epochs\n",
    "    print(f\"\\n================ EPOCH {epoch} ================\")\n",
    "    \n",
    "    # G·ªçi h√†m train v√† nh·∫≠n v·ªÅ 4 gi√° tr·ªã\n",
    "    avg_loss, prec, rec, f1 = train_one_epoch()\n",
    "    \n",
    "    print(f\"‚úÖ K·∫æT TH√öC EPOCH {epoch}\")\n",
    "    print(f\"   - Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"   - Precision:    {prec:.4f}\")\n",
    "    print(f\"   - Recall:       {rec:.4f}\")\n",
    "    print(f\"   - F1-Score:     {f1:.4f}\")\n",
    "\n",
    "print(\"\\nüéâ ƒê√£ train xong! (Model saved)\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. L∆ØU MODEL & EMBEDDINGS\n",
    "# ---------------------------------------------------------\n",
    "torch.save(model.state_dict(), \"HGNN_Model.pth\")\n",
    "torch.save(drug_emb.state_dict(), \"Drug_Embeddings.pth\")\n",
    "torch.save(react_emb.state_dict(), \"Reaction_Embeddings.pth\")\n",
    "torch.save(patient_lin.state_dict(), \"Patient_Encoder.pth\")\n",
    "# L∆∞u danh s√°ch t√™n thu·ªëc/ph·∫£n ·ª©ng ƒë·ªÉ map l·∫°i sau n√†y\n",
    "import joblib\n",
    "joblib.dump(drug_cols, \"Drug_Names.pkl\")\n",
    "joblib.dump(reaction_cols, \"Reaction_Names.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
