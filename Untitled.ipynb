{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e17070fb-7ef6-4c6d-ac25-6cf6494632ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Features: (661271, 5286)\n",
      "Targets:  (661271, 10488)\n",
      "Data successfully split.\n",
      "Train size: 529016\n",
      "Test size:  132255\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 1. SETUP PATHS\n",
    "# Using the paths found in your provided notebook\n",
    "FOLDER_PATH = \"/home/juan/Work/Midterm project/splited/\"\n",
    "X_PATH = os.path.join(FOLDER_PATH, \"X_data.npz\")\n",
    "Y_PATH = os.path.join(FOLDER_PATH, \"Y_data.npz\")\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "# Load the sparse matrices\n",
    "X = sparse.load_npz(X_PATH)\n",
    "Y = sparse.load_npz(Y_PATH)\n",
    "\n",
    "print(f\"Features: {X.shape}\")\n",
    "print(f\"Targets:  {Y.shape}\")\n",
    "\n",
    "# 2. SPLIT DATA\n",
    "# We split indices first, then slice the sparse matrix to keep it memory efficient\n",
    "# random_state=42 ensures the split is the same every time you run it\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data successfully split.\")\n",
    "print(f\"Train size: {X_train.shape[0]}\")\n",
    "print(f\"Test size:  {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b1054d3-a29c-4154-b096-d2e83f4e430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataLoaders...\n",
      "DataLoaders ready.\n"
     ]
    }
   ],
   "source": [
    "# 3. DEFINE DATASET CLASS\n",
    "class GraphDrugDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X \n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Extract one row\n",
    "        row = self.X[idx]\n",
    "        \n",
    "        # Convert to dense tensor just for this single row (saves RAM vs converting whole dataset)\n",
    "        full_row = torch.tensor(row.toarray(), dtype=torch.float32).squeeze()\n",
    "        target_row = torch.tensor(self.Y[idx].toarray(), dtype=torch.float32).squeeze()\n",
    "        \n",
    "        return full_row, target_row\n",
    "\n",
    "# 4. CREATE DATALOADERS\n",
    "print(\"Creating DataLoaders...\")\n",
    "batch_size = 128 # Adjust based on your GPU VRAM\n",
    "\n",
    "train_dataset = GraphDrugDataset(X_train, Y_train)\n",
    "test_dataset = GraphDrugDataset(X_test, Y_test)\n",
    "\n",
    "# num_workers=0 is safest for debugging. Increase to 2 or 4 for speed later.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"DataLoaders ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3dda00d-111f-4551-94a0-dabc5805c6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 5. DEFINE THE MODEL\n",
    "class BipartiteGNN(nn.Module):\n",
    "    def __init__(self, num_drugs, num_demographics, output_dim, embed_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # A. Learnable vector for each Drug (replaces the sparse 0/1 columns)\n",
    "        # This compresses 5000+ drugs into a semantic vector space\n",
    "        self.drug_embeddings = nn.Embedding(num_drugs, embed_dim)\n",
    "        \n",
    "        # B. Prediction Layers\n",
    "        # Input size is: Drug_Embedding_Dim + Demographic_Features\n",
    "        self.fc1 = nn.Linear(embed_dim + num_demographics, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, output_dim) # Output = Number of possible adverse events\n",
    "        \n",
    "    def forward(self, x_sparse_drugs, x_dense_demog):\n",
    "        # 1. AGGREGATE DRUGS (The GNN Step)\n",
    "        # We multiply the sparse patient matrix (0s and 1s) by the drug embeddings.\n",
    "        # This sums up the embeddings of all drugs a patient took.\n",
    "        # Math: (Batch x Num_Drugs) @ (Num_Drugs x Embed_Dim) -> (Batch x Embed_Dim)\n",
    "        patient_drug_profile = torch.sparse.mm(x_sparse_drugs, self.drug_embeddings.weight)\n",
    "        \n",
    "        # 2. COMBINE WITH DEMOGRAPHICS\n",
    "        combined = torch.cat([patient_drug_profile, x_dense_demog], dim=1)\n",
    "        \n",
    "        # 3. CLASSIFY\n",
    "        x = F.relu(self.bn1(self.fc1(combined)))\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        logits = self.fc2(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Initialize the Model\n",
    "# According to your R script, columns 0, 1, 2 are Age, AgeGroup, Gender.\n",
    "# So NUM_DEMOGRAPHICS = 3.\n",
    "NUM_DEMOG = 3\n",
    "NUM_DRUGS = X_train.shape[1] - NUM_DEMOG\n",
    "OUTPUT_DIM = Y_train.shape[1]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = BipartiteGNN(NUM_DRUGS, NUM_DEMOG, OUTPUT_DIM).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b42c2f66-9718-4b63-a35c-724b59a62815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_batch_metrics(logits, targets, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Calculates TP, FP, FN on GPU for a single batch.\n",
    "    Returns the counts, not the lists.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > threshold).float()\n",
    "        \n",
    "        # Micro-Average logic (flatten everyone into one big list of events)\n",
    "        # TP: Predicted 1, Actual 1\n",
    "        tp = (preds * targets).sum().item()\n",
    "        \n",
    "        # FP: Predicted 1, Actual 0\n",
    "        fp = (preds * (1 - targets)).sum().item()\n",
    "        \n",
    "        # FN: Predicted 0, Actual 1\n",
    "        fn = ((1 - preds) * targets).sum().item()\n",
    "        \n",
    "    return tp, fp, fn\n",
    "\n",
    "def get_f1(tp, fp, fn):\n",
    "    \"\"\"Computes F1 from counts to avoid ZeroDivisionError\"\"\"\n",
    "    epsilon = 1e-7\n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0273e2ec-fdf3-451d-9166-260ebbb63fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• GPU GRAPH MODE (Balanced) | Device: cuda\n",
      "Calculating Balanced Weights...\n",
      "Weights Calculated. Mean: 15.00 | Max: 15.0\n",
      "\n",
      "üöÄ STARTING BALANCED TRAINING...\n",
      "Ep 1 | Batch 120 | Loss: 0.0147\n",
      "--> Epoch 1 Done. Loss: 0.0451\n",
      "Ep 2 | Batch 120 | Loss: 0.0123\n",
      "--> Epoch 2 Done. Loss: 0.0134\n",
      "Ep 3 | Batch 120 | Loss: 0.0112\n",
      "--> Epoch 3 Done. Loss: 0.0118\n",
      "Ep 4 | Batch 120 | Loss: 0.0111\n",
      "--> Epoch 4 Done. Loss: 0.0111\n",
      "Ep 5 | Batch 120 | Loss: 0.0106\n",
      "--> Epoch 5 Done. Loss: 0.0107\n",
      "Ep 6 | Batch 120 | Loss: 0.0104\n",
      "--> Epoch 6 Done. Loss: 0.0105\n",
      "Ep 7 | Batch 120 | Loss: 0.0102\n",
      "--> Epoch 7 Done. Loss: 0.0103\n",
      "Ep 8 | Batch 120 | Loss: 0.0103\n",
      "--> Epoch 8 Done. Loss: 0.0101\n",
      "Ep 9 | Batch 120 | Loss: 0.0099\n",
      "--> Epoch 9 Done. Loss: 0.0100\n",
      "Ep 10 | Batch 120 | Loss: 0.0100\n",
      "--> Epoch 10 Done. Loss: 0.0099\n",
      "\n",
      "Scanning thresholds on GPU (Streaming Mode)...\n",
      "\n",
      "--- THRESHOLD RESULTS ---\n",
      "Threshold  | F1         | Precision  | Recall    \n",
      "--------------------------------------------------\n",
      "0.10       | 0.0571     | 0.0298     | 0.6692\n",
      "0.15       | 0.0749     | 0.0399     | 0.5988\n",
      "0.20       | 0.0925     | 0.0506     | 0.5370\n",
      "0.25       | 0.1094     | 0.0616     | 0.4852\n",
      "0.30       | 0.1234     | 0.0717     | 0.4409\n",
      "0.35       | 0.1371     | 0.0828     | 0.3983\n",
      "0.40       | 0.1506     | 0.0956     | 0.3545\n",
      "0.45       | 0.1620     | 0.1090     | 0.3151\n",
      "0.50       | 0.1707     | 0.1227     | 0.2804\n",
      "0.55       | 0.1755     | 0.1367     | 0.2449\n",
      "0.60       | 0.1773     | 0.1531     | 0.2104\n",
      "0.65       | 0.1727     | 0.1732     | 0.1722\n",
      "0.70       | 0.1596     | 0.2068     | 0.1300\n",
      "0.75       | 0.1366     | 0.2469     | 0.0944\n",
      "0.80       | 0.1031     | 0.3054     | 0.0620\n",
      "0.85       | 0.0711     | 0.3727     | 0.0393\n",
      "--------------------------------------------------\n",
      "üèÜ BEST CONFIG: Threshold 0.60\n",
      "   F1 Score:  0.1773\n",
      "   Precision: 0.1531\n",
      "   Recall:    0.2104\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.amp as amp\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "# We define PHYSICAL_BATCH_SIZE here to match the code below\n",
    "PHYSICAL_BATCH_SIZE = 4096      \n",
    "ACCUM_STEPS = 4        \n",
    "EPOCHS = 10            \n",
    "LR = 0.001             \n",
    "\n",
    "# Enable TF32 for Speed\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üî• GPU GRAPH MODE (Balanced) | Device: {device}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. TUNED CLASS WEIGHTS (Less Aggressive)\n",
    "# ==========================================\n",
    "print(\"Calculating Balanced Weights...\")\n",
    "# Ensure Y_train exists (from previous cells). If not, reload data first.\n",
    "y_freq = np.array(Y_train.sum(axis=0)).flatten()\n",
    "total_samples = Y_train.shape[0]\n",
    "\n",
    "# Weight Formula\n",
    "pos_weights = (total_samples - y_freq) / (y_freq + 1e-5)\n",
    "\n",
    "# --- THE FIX: Lower the Cap ---\n",
    "# Cap at 15.0 to balance Precision/Recall\n",
    "pos_weights = np.clip(pos_weights, 1.0, 15.0) \n",
    "\n",
    "pos_weight_tensor = torch.from_numpy(pos_weights).float().to(device)\n",
    "print(f\"Weights Calculated. Mean: {pos_weights.mean():.2f} | Max: {pos_weights.max()}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. FAST LOADER\n",
    "# ==========================================\n",
    "class FastSparseLoader:\n",
    "    def __init__(self, X, Y, batch_size=1024, shuffle=True):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_samples = X.shape[0]\n",
    "        self.num_batches = int(np.ceil(self.num_samples / batch_size))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        indices = np.arange(self.num_samples)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        for i in range(0, self.num_samples, self.batch_size):\n",
    "            batch_indices = indices[i : i + self.batch_size]\n",
    "            x_tensor = torch.from_numpy(self.X[batch_indices].toarray()).float()\n",
    "            y_tensor = torch.from_numpy(self.Y[batch_indices].toarray()).float()\n",
    "            yield x_tensor, y_tensor\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "# ==========================================\n",
    "# 4. GRAPH NETWORK\n",
    "# ==========================================\n",
    "class GraphGNN(nn.Module):\n",
    "    def __init__(self, num_drugs, num_demog, output_dim, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.drug_embeddings = nn.Embedding(num_drugs, embed_dim)\n",
    "        self.graph_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.input_proj = nn.Linear(embed_dim + num_demog, 2048)\n",
    "        self.bn_in = nn.BatchNorm1d(2048)\n",
    "        \n",
    "        self.res1 = nn.Sequential(nn.Linear(2048, 2048), nn.LayerNorm(2048), nn.GELU())\n",
    "        self.res2 = nn.Sequential(nn.Linear(2048, 2048), nn.LayerNorm(2048), nn.GELU())\n",
    "        \n",
    "        self.output = nn.Linear(2048, output_dim)\n",
    "        \n",
    "    def forward(self, x_sparse_drugs, x_dense_demog):\n",
    "        with torch.amp.autocast('cuda', enabled=False):\n",
    "            x_sparse = x_sparse_drugs.float()\n",
    "            weights = self.drug_embeddings.weight.float()\n",
    "            patient_drug_sum = torch.sparse.mm(x_sparse, weights)\n",
    "            \n",
    "        patient_drug_features = self.graph_norm(patient_drug_sum)\n",
    "        combined = torch.cat([patient_drug_features, x_dense_demog], dim=1)\n",
    "        \n",
    "        x = F.gelu(self.bn_in(self.input_proj(combined)))\n",
    "        x = x + self.res1(x)\n",
    "        x = x + self.res2(x)\n",
    "        return self.output(x)\n",
    "\n",
    "# ==========================================\n",
    "# 5. TRAINING LOOP\n",
    "# ==========================================\n",
    "NUM_DEMOG = 2 \n",
    "NUM_DRUGS = X_train.shape[1] - NUM_DEMOG\n",
    "OUTPUT_DIM = Y_train.shape[1]\n",
    "\n",
    "model = GraphGNN(NUM_DRUGS, NUM_DEMOG, OUTPUT_DIM).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# Corrected Variable usage here\n",
    "train_loader = FastSparseLoader(X_train, Y_train, batch_size=PHYSICAL_BATCH_SIZE, shuffle=True)\n",
    "test_loader = FastSparseLoader(X_test, Y_test, batch_size=PHYSICAL_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nüöÄ STARTING BALANCED TRAINING...\")\n",
    "total_start = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for i, (features, targets) in enumerate(train_loader):\n",
    "        features = features.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        x_demog = features[:, :NUM_DEMOG]\n",
    "        x_drugs_sparse = features[:, NUM_DEMOG:].to_sparse()\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            logits = model(x_drugs_sparse, x_demog)\n",
    "            loss = criterion(logits, targets) / ACCUM_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % ACCUM_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        running_loss += loss.item() * ACCUM_STEPS\n",
    "        \n",
    "        if i % 20 == 0 and i > 0:\n",
    "            print(f\"\\rEp {epoch+1} | Batch {i} | Loss: {loss.item()*ACCUM_STEPS:.4f}\", end=\"\")\n",
    "\n",
    "    print(f\"\\n--> Epoch {epoch+1} Done. Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. MEMORY-SAFE THRESHOLD SEARCH\n",
    "# ==========================================\n",
    "print(\"\\nScanning thresholds on GPU (Streaming Mode)...\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Define the thresholds we want to test\n",
    "thresholds = torch.arange(0.1, 0.9, 0.05).to(device)\n",
    "num_th = len(thresholds)\n",
    "\n",
    "# Store TP, FP, FN for each threshold\n",
    "stats = torch.zeros((num_th, 3), device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (features, targets) in enumerate(test_loader):\n",
    "        features = features.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        \n",
    "        x_demog = features[:, :NUM_DEMOG]\n",
    "        x_drugs_sparse = features[:, NUM_DEMOG:].to_sparse()\n",
    "        \n",
    "        # 1. Forward Pass\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            logits = model(x_drugs_sparse, x_demog)\n",
    "            probs = torch.sigmoid(logits)\n",
    "        \n",
    "        # 2. Check ALL thresholds for this batch\n",
    "        for t_idx, th in enumerate(thresholds):\n",
    "            preds = (probs > th).float()\n",
    "            \n",
    "            # Sum batch immediately to save RAM\n",
    "            tp = (preds * targets).sum()\n",
    "            fp = (preds * (1 - targets)).sum()\n",
    "            fn = ((1 - preds) * targets).sum()\n",
    "            \n",
    "            stats[t_idx, 0] += tp\n",
    "            stats[t_idx, 1] += fp\n",
    "            stats[t_idx, 2] += fn\n",
    "            \n",
    "        # 3. Clean up\n",
    "        del logits, probs, preds, features, targets, x_demog, x_drugs_sparse\n",
    "\n",
    "# ==========================================\n",
    "# 7. FIND WINNER\n",
    "# ==========================================\n",
    "best_f1 = 0\n",
    "best_th = 0\n",
    "best_metrics = (0, 0) # P, R\n",
    "\n",
    "print(\"\\n--- THRESHOLD RESULTS ---\")\n",
    "print(f\"{'Threshold':<10} | {'F1':<10} | {'Precision':<10} | {'Recall':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for t_idx, th in enumerate(thresholds):\n",
    "    tp = stats[t_idx, 0]\n",
    "    fp = stats[t_idx, 1]\n",
    "    fn = stats[t_idx, 2]\n",
    "    \n",
    "    epsilon = 1e-7\n",
    "    p = tp / (tp + fp + epsilon)\n",
    "    r = tp / (tp + fn + epsilon)\n",
    "    f1 = 2 * (p * r) / (p + r + epsilon)\n",
    "    \n",
    "    print(f\"{th:.2f}       | {f1:.4f}     | {p:.4f}     | {r:.4f}\")\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1.item()\n",
    "        best_th = th.item()\n",
    "        best_metrics = (p.item(), r.item())\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"üèÜ BEST CONFIG: Threshold {best_th:.2f}\")\n",
    "print(f\"   F1 Score:  {best_f1:.4f}\")\n",
    "print(f\"   Precision: {best_metrics[0]:.4f}\")\n",
    "print(f\"   Recall:    {best_metrics[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c597a15b-d3a6-441d-9a49-fb32ff4b2d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ STARTING INTERACTION TRAINING...\n",
      "Ep 1 | Batch 120 | Loss: 0.0149\n",
      "--> Epoch 1 Done. Loss: 0.0448\n",
      "Ep 2 | Batch 120 | Loss: 0.0122\n",
      "--> Epoch 2 Done. Loss: 0.0132\n",
      "Ep 3 | Batch 120 | Loss: 0.0116\n",
      "--> Epoch 3 Done. Loss: 0.0117\n",
      "Ep 4 | Batch 120 | Loss: 0.0110\n",
      "--> Epoch 4 Done. Loss: 0.0110\n",
      "Ep 5 | Batch 120 | Loss: 0.0105\n",
      "--> Epoch 5 Done. Loss: 0.0106\n",
      "Ep 6 | Batch 120 | Loss: 0.0103\n",
      "--> Epoch 6 Done. Loss: 0.0104\n",
      "Ep 7 | Batch 120 | Loss: 0.0104\n",
      "--> Epoch 7 Done. Loss: 0.0102\n",
      "Ep 8 | Batch 120 | Loss: 0.0100\n",
      "--> Epoch 8 Done. Loss: 0.0100\n",
      "Ep 9 | Batch 120 | Loss: 0.0100\n",
      "--> Epoch 9 Done. Loss: 0.0099\n",
      "Ep 10 | Batch 120 | Loss: 0.0096\n",
      "--> Epoch 10 Done. Loss: 0.0097\n",
      "Ep 11 | Batch 120 | Loss: 0.0095\n",
      "--> Epoch 11 Done. Loss: 0.0096\n",
      "Ep 12 | Batch 120 | Loss: 0.0096\n",
      "--> Epoch 12 Done. Loss: 0.0095\n",
      "Ep 13 | Batch 120 | Loss: 0.0094\n",
      "--> Epoch 13 Done. Loss: 0.0094\n",
      "Ep 14 | Batch 120 | Loss: 0.0092\n",
      "--> Epoch 14 Done. Loss: 0.0093\n",
      "Ep 15 | Batch 120 | Loss: 0.0092\n",
      "--> Epoch 15 Done. Loss: 0.0092\n",
      "\n",
      "Scanning thresholds on GPU...\n",
      "----------------------------------------\n",
      "Thresh   | F1       | Prec     | Rec     \n",
      "----------------------------------------\n",
      "0.10     | 0.0598   | 0.0313   | 0.6673\n",
      "0.15     | 0.0768   | 0.0410   | 0.6001\n",
      "0.20     | 0.0936   | 0.0512   | 0.5414\n",
      "0.25     | 0.1084   | 0.0609   | 0.4944\n",
      "0.30     | 0.1227   | 0.0711   | 0.4500\n",
      "0.35     | 0.1358   | 0.0814   | 0.4096\n",
      "0.40     | 0.1474   | 0.0918   | 0.3728\n",
      "0.45     | 0.1588   | 0.1040   | 0.3354\n",
      "0.50     | 0.1687   | 0.1177   | 0.2977\n",
      "0.55     | 0.1761   | 0.1330   | 0.2603\n",
      "----------------------------------------\n",
      "üèÜ Best F1: 0.1761 @ Threshold 0.55\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.amp as amp\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "PHYSICAL_BATCH_SIZE = 4096      \n",
    "ACCUM_STEPS = 4        \n",
    "EPOCHS = 15            \n",
    "LR = 0.001             \n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. WEIGHTS (Keep your Balanced 15.0 Cap)\n",
    "# ==========================================\n",
    "# (Re-running this block to ensure variables exist)\n",
    "y_freq = np.array(Y_train.sum(axis=0)).flatten()\n",
    "total_samples = Y_train.shape[0]\n",
    "pos_weights = (total_samples - y_freq) / (y_freq + 1e-5)\n",
    "pos_weights = np.clip(pos_weights, 1.0, 15.0) \n",
    "pos_weight_tensor = torch.from_numpy(pos_weights).float().to(device)\n",
    "\n",
    "# ==========================================\n",
    "# 3. INTERACTION GNN (Factorization Machine)\n",
    "# ==========================================\n",
    "class InteractionGNN(nn.Module):\n",
    "    def __init__(self, num_drugs, num_demog, output_dim, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.drug_embeddings = nn.Embedding(num_drugs, embed_dim)\n",
    "        self.graph_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # 1. Standard Linear Part (The \"Sum\")\n",
    "        self.linear_part = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # 2. Deep Part (Process the interactions)\n",
    "        # Input is Embed_Dim (from Sum) + Embed_Dim (from Interactions) + Demog\n",
    "        input_width = (embed_dim * 2) + num_demog\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_width, 2048)\n",
    "        self.bn_in = nn.BatchNorm1d(2048)\n",
    "        \n",
    "        # Deep Residual Layers\n",
    "        self.res1 = nn.Sequential(nn.Linear(2048, 2048), nn.LayerNorm(2048), nn.GELU())\n",
    "        self.res2 = nn.Sequential(nn.Linear(2048, 2048), nn.LayerNorm(2048), nn.GELU())\n",
    "        \n",
    "        self.output = nn.Linear(2048, output_dim)\n",
    "        \n",
    "    def forward(self, x_sparse_drugs, x_dense_demog):\n",
    "        with torch.amp.autocast('cuda', enabled=False):\n",
    "            # A. Get Embeddings\n",
    "            x_sparse = x_sparse_drugs.float()\n",
    "            embeddings = self.drug_embeddings.weight.float()\n",
    "            \n",
    "            # B. First Order: Sum of Embeddings (Standard GNN)\n",
    "            # Sum(v_i)\n",
    "            sum_embeddings = torch.sparse.mm(x_sparse, embeddings)\n",
    "            \n",
    "            # C. Second Order: Interactions (Factorization Machine Logic)\n",
    "            # 0.5 * [ (Sum v_i)^2 - Sum (v_i^2) ]\n",
    "            \n",
    "            # Term 1: Square of the sum\n",
    "            sum_squared = torch.square(sum_embeddings)\n",
    "            \n",
    "            # Term 2: Sum of squares\n",
    "            # We need sparse MM for squared weights: x_sparse @ (weights^2)\n",
    "            squared_embeddings = torch.square(embeddings)\n",
    "            squared_sum = torch.sparse.mm(x_sparse, squared_embeddings)\n",
    "            \n",
    "            # The Interaction Vector\n",
    "            interactions = 0.5 * (sum_squared - squared_sum)\n",
    "            \n",
    "        # D. Normalize & Combine\n",
    "        # We now have two rich feature vectors: Linear effects & Interaction effects\n",
    "        norm_sum = self.graph_norm(sum_embeddings)\n",
    "        norm_inter = self.graph_norm(interactions)\n",
    "        \n",
    "        combined = torch.cat([norm_sum, norm_inter, x_dense_demog], dim=1)\n",
    "        \n",
    "        # E. Deep Network\n",
    "        x = F.gelu(self.bn_in(self.input_proj(combined)))\n",
    "        x = x + self.res1(x)\n",
    "        x = x + self.res2(x)\n",
    "        \n",
    "        return self.output(x)\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING SETUP\n",
    "# ==========================================\n",
    "NUM_DEMOG = 2 \n",
    "NUM_DRUGS = X_train.shape[1] - NUM_DEMOG\n",
    "OUTPUT_DIM = Y_train.shape[1]\n",
    "\n",
    "# Init the Upgrade\n",
    "model = InteractionGNN(NUM_DRUGS, NUM_DEMOG, OUTPUT_DIM).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# Loaders\n",
    "train_loader = FastSparseLoader(X_train, Y_train, batch_size=PHYSICAL_BATCH_SIZE, shuffle=True)\n",
    "test_loader = FastSparseLoader(X_test, Y_test, batch_size=PHYSICAL_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nüöÄ STARTING INTERACTION TRAINING...\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. EXECUTE TRAINING\n",
    "# ==========================================\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for i, (features, targets) in enumerate(train_loader):\n",
    "        features = features.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        x_demog = features[:, :NUM_DEMOG]\n",
    "        x_drugs_sparse = features[:, NUM_DEMOG:].to_sparse()\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            logits = model(x_drugs_sparse, x_demog)\n",
    "            loss = criterion(logits, targets) / ACCUM_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % ACCUM_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        running_loss += loss.item() * ACCUM_STEPS\n",
    "        \n",
    "        if i % 20 == 0 and i > 0:\n",
    "            print(f\"\\rEp {epoch+1} | Batch {i} | Loss: {loss.item()*ACCUM_STEPS:.4f}\", end=\"\")\n",
    "\n",
    "    print(f\"\\n--> Epoch {epoch+1} Done. Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. EVALUATION\n",
    "# ==========================================\n",
    "print(\"\\nScanning thresholds on GPU...\")\n",
    "model.eval()\n",
    "\n",
    "thresholds = torch.arange(0.1, 0.6, 0.05).to(device) # Focused search range\n",
    "stats = torch.zeros((len(thresholds), 3), device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, targets in test_loader:\n",
    "        features = features.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        x_demog = features[:, :NUM_DEMOG]\n",
    "        x_drugs_sparse = features[:, NUM_DEMOG:].to_sparse()\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            logits = model(x_drugs_sparse, x_demog)\n",
    "            probs = torch.sigmoid(logits)\n",
    "        \n",
    "        for t_idx, th in enumerate(thresholds):\n",
    "            preds = (probs > th).float()\n",
    "            stats[t_idx, 0] += (preds * targets).sum()\n",
    "            stats[t_idx, 1] += (preds * (1 - targets)).sum()\n",
    "            stats[t_idx, 2] += ((1 - preds) * targets).sum()\n",
    "\n",
    "best_f1 = 0\n",
    "best_th = 0\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Thresh':<8} | {'F1':<8} | {'Prec':<8} | {'Rec':<8}\")\n",
    "print(\"-\" * 40)\n",
    "for i, th in enumerate(thresholds):\n",
    "    tp, fp, fn = stats[i]\n",
    "    p = tp / (tp + fp + 1e-7)\n",
    "    r = tp / (tp + fn + 1e-7)\n",
    "    f1 = 2 * p * r / (p + r + 1e-7)\n",
    "    print(f\"{th:.2f}     | {f1:.4f}   | {p:.4f}   | {r:.4f}\")\n",
    "    if f1 > best_f1: best_f1, best_th = f1, th\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"üèÜ Best F1: {best_f1:.4f} @ Threshold {best_th:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3122d67b-bea9-4dbc-be2b-cc8ed707aa20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
